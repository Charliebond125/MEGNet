{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYpOqs7nkde0",
        "outputId": "e8bb2290-118d-474b-9657-4aff3114f448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MEGNet'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 48 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (48/48), 24.32 MiB | 7.69 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Charliebond125/MEGNet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPLeSvqLwofX",
        "outputId": "36f1ed1d-c6fb-4de4-fa5f-dbc1fdb1f12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from mne) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from mne) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.4.0->mne) (2.8.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T2UbN4jHbTxK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mne\n",
        "import random\n",
        "from mne import read_epochs\n",
        "from mne import pick_types, Epochs\n",
        "from mne.channels import read_layout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import utils as np_utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "import sys, os\n",
        "\n",
        "from MEGNet.MEGModels import MEGNet, ShallowConvNet, DeepConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vTYWouxJbWI0"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the data for a specific event pair\n",
        "def preprocess_data(event_pair_function, epochs_data):\n",
        "    epochs_data_selected = event_pair_function(epochs_data)\n",
        "\n",
        "    # Select only gradiometer channels\n",
        "    grad_channels = mne.pick_types(epochs_data_selected.info, meg='grad')\n",
        "    grad_channel_names = [epochs_data_selected.ch_names[ch] for ch in grad_channels]\n",
        "    epochs_data_selected = epochs_data_selected.pick_channels(grad_channel_names)\n",
        "\n",
        "    return epochs_data_selected\n",
        "\n",
        "# Your provided functions for each event pair\n",
        "def hands_vs_feet(epochs_data):\n",
        "    event_dict = [\n",
        "        \"hand_imagery\",\n",
        "        \"feet_imagery\"\n",
        "    ]\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "\n",
        "def hands_vs_word(epochs_data):\n",
        "    event_dict = [\n",
        "        \"hand_imagery\",\n",
        "        \"word_imagery\"\n",
        "    ]\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "def hands_vs_sub(epochs_data):\n",
        "    event_dict = [\n",
        "        \"hand_imagery\",\n",
        "        \"subtraction_imagery\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "def feet_vs_word(epochs_data):\n",
        "    event_dict = [\n",
        "        \"feet_imagery\",\n",
        "        \"word_imagery\"\n",
        "    ]\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "def feet_vs_sub(epochs_data):\n",
        "    event_dict = [\n",
        "        \"feet_imagery\",\n",
        "        \"subtraction_imagery\"\n",
        "    ]\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "def word_vs_sub(epochs_data):\n",
        "    event_dict = [\n",
        "        \"word_imagery\",\n",
        "        \"subtraction_imagery\"\n",
        "    ]\n",
        "\n",
        "    return epochs_data[event_dict]\n",
        "\n",
        "def min_max_scaler(epoch_data):\n",
        "    # scales data to -1, 1\n",
        "    max_abs = np.max(np.abs(epoch_data))\n",
        "    processed_data = epoch_data / max_abs\n",
        "    return processed_data\n",
        "\n",
        "def mne_standard_scaler(epoch_data):\n",
        "    # Different from sklearn standardscaler\n",
        "    # Works via channel to channel\n",
        "\n",
        "    from mne.decoding import Scaler\n",
        "    scaler = Scaler(scalings=\"mean\")\n",
        "    scaled_data = scaler.fit_transform(epoch_data)\n",
        "    return scaled_data\n",
        "\n",
        "def z_score_scaler(epoch_data):\n",
        "    # scales by removing mean and unit standard deviation\n",
        "    mean = np.mean(epoch_data)\n",
        "    std = np.std(epoch_data)\n",
        "    processed_data = (epoch_data - mean) / std\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYdTwRE4OJ_3"
      },
      "source": [
        "Load the combined epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oac7Qq5HOH5J"
      },
      "outputs": [],
      "source": [
        "# Define epochs for training\n",
        "n_epochs = 20\n",
        "\n",
        "out_f_name = '/content' +'acc_EEGNet' + '_epochs_' + str(n_epochs) + '_filtered_' + 'sub.npy'\n",
        "print(out_f_name)\n",
        "\n",
        "# loading combined epochs data\n",
        "print(\"Loading epoched data...\")\n",
        "epoched_data = mne.read_epochs('/content/drive/MyDrive/MEG/Maxwell Filtered Data/Combined Run data/Sub_11_combined_runs_epoched_data-epo.fif', preload=True)\n",
        "print(f\"Epoched data successfully loaded {epoched_data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train MEGNet model"
      ],
      "metadata": {
        "id": "Y3pBO6-LCkfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing kernel length values\n",
        "kl_values = [8,16, 32, 64, 128]\n",
        "\n",
        "# Controlling dropout values\n",
        "d_values = [0.1, 0.2, 0.5]\n",
        "\n",
        "# Initialize the DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['Event Pair', 'Best kl', 'Best d', 'Best Accuracy','Best AUC Score', 'Best F1 Score'])\n",
        "\n",
        "# List of event pairs and their corresponding selection functions\n",
        "event_pairs = [\n",
        "    ('hands_vs_word', hands_vs_word),\n",
        "    ('hands_vs_sub', hands_vs_sub),\n",
        "    ('feet_vs_word', feet_vs_word),\n",
        "    ('feet_vs_sub', feet_vs_sub),\n",
        "    ('hands_vs_feet', hands_vs_feet),\n",
        "    ('word_vs_sub', word_vs_sub)\n",
        "\n",
        "]\n",
        "\n",
        "# Iterate over the event pairs\n",
        "for event_pair_name, event_pair_function in event_pairs:\n",
        "    print(f\"Training for event pair: {event_pair_name}\")\n",
        "\n",
        "    # Preprocess the data for the current event pair\n",
        "    epochs_data_selected = preprocess_data(event_pair_function, epoched_data)\n",
        "\n",
        "    # Extract data and labels\n",
        "    X = epochs_data_selected.get_data()\n",
        "    y = epochs_data_selected.events[:, -1]\n",
        "\n",
        "    # Map the event codes to integer labels starting from 0\n",
        "    unique_events = np.unique(y)\n",
        "    event_to_label = {event: idx for idx, event in enumerate(unique_events)}\n",
        "    y_labels = np.array([event_to_label[event] for event in y])\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    y_one_hot = to_categorical(y_labels, num_classes=2)\n",
        "\n",
        "\n",
        "    # Split the data into training and testing sets (95% for training, 5% for testing)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.05, random_state=42, stratify=y_one_hot)\n",
        "\n",
        "\n",
        "    # Print class distribution for training set\n",
        "    train_class_distribution = pd.Series(y_train.argmax(axis=1)).value_counts(normalize=True)\n",
        "    print(\"Class Distribution in Training Set:\")\n",
        "    print(train_class_distribution)\n",
        "\n",
        "    # Print class distribution for testing set\n",
        "    test_class_distribution = pd.Series(y_test.argmax(axis=1)).value_counts(normalize=True)\n",
        "    print(\"Class Distribution in Testing Set:\")\n",
        "    print(test_class_distribution)\n",
        "\n",
        "    # Normalize the data using min-max scaling\n",
        "    X_train = min_max_scaler(X_train)\n",
        "    X_test = min_max_scaler(X_test)\n",
        "\n",
        "    # Reshape the data to match EEGNet input format\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
        "\n",
        "\n",
        "    # Update the nb_classes parameter in the EEGNet function\n",
        "    nb_classes = y_train.shape[1]\n",
        "\n",
        "    # Initialize variables to store the highest AUC score and corresponding kl, d values for the current event pair\n",
        "    highest_auc_score = 0\n",
        "    highest_acc = 0\n",
        "    f1 = 0\n",
        "    best_kl = None\n",
        "    best_d = None\n",
        "\n",
        "    # Iterate over the kl and d values\n",
        "    for kl in kl_values:\n",
        "        for d in d_values:\n",
        "            print()\n",
        "            print(f\"Current kl: {kl}\")\n",
        "            print(f\"Current d: {d}\")\n",
        "            print()\n",
        "\n",
        "            # Configure the MEGNet model\n",
        "            # The model will be trained and tested with randomized class assignments for each  sample.\n",
        "            model = MEGNet(nb_classes=nb_classes, Chans=X_train.shape[1], Samples=X_train.shape[2], dropoutRate=d, kernLength=kl)\n",
        "            # print(model.summary())\n",
        "            # Compile the model\n",
        "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "            # Set a valid path for model checkpoints\n",
        "            checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "            # Class weights\n",
        "            class_weights = {0: 1, 1: 1}\n",
        "\n",
        "            # Fit the model to the training data\n",
        "            history = model.fit(X_train, y_train, batch_size=16, epochs=n_epochs, verbose=2,\n",
        "                                validation_split=0.2, callbacks=[checkpointer, EarlyStopping(patience=5)],\n",
        "                                class_weight=class_weights)\n",
        "\n",
        "            # Evaluate the model on the test data\n",
        "            y_pred = model.predict(X_test)\n",
        "            auc_score = roc_auc_score(y_test, y_pred)\n",
        "            acc = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "            f1_score_val = f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
        "\n",
        "            # Print the Accuracy, AUC and F1 scores\n",
        "            print('Accuracy Score:', acc)\n",
        "            print('AUC Accuracy Score:', auc_score)\n",
        "            print('F1 Score:', f1_score_val)\n",
        "\n",
        "            # Check if the current AUC score is higher than the highest recorded score for the current event pair\n",
        "            if acc > highest_acc:\n",
        "                highest_auc_score = auc_score\n",
        "                best_kl = kl\n",
        "                best_d = d\n",
        "                f1 = f1_score_val\n",
        "                highest_acc = acc\n",
        "\n",
        "            # Reset the Keras session to clear the model\n",
        "            K.clear_session()\n",
        "\n",
        "            print()\n",
        "            print('=' * 100)\n",
        "            print('=' * 100)\n",
        "            print()\n",
        "\n",
        "    # Append the best parameters and scores for the current event pair to the results DataFrame\n",
        "    results_df = results_df.append({'Event Pair': event_pair_name, 'Best kl': best_kl, 'Best d': best_d,\n",
        "                                    'Best Accuracy':highest_acc,'Best AUC Score': highest_auc_score,\n",
        "                                    'Best F1 Score': f1},ignore_index=True)\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "Lx9wyEQ3K9CY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ShallowConvNet or DeepConvNet"
      ],
      "metadata": {
        "id": "31E_X_obW0w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlling dropout values\n",
        "d_values = [0.5, 0.6, 0.8]\n",
        "\n",
        "# Initialize the DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['Event Pair', 'Best d','Best Accuracy',  'Best AUC Score', 'Best F1 Score'])\n",
        "\n",
        "# List of event pairs and their corresponding selection functions\n",
        "event_pairs = [\n",
        "    ('hands_vs_word', hands_vs_word),\n",
        "    ('hands_vs_sub', hands_vs_sub),\n",
        "    ('feet_vs_word', feet_vs_word),\n",
        "    ('feet_vs_sub', feet_vs_sub),\n",
        "    ('hands_vs_feet', hands_vs_feet),\n",
        "    ('word_vs_sub', word_vs_sub)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Iterate over the event pairs\n",
        "for event_pair_name, event_pair_function in event_pairs:\n",
        "    print(f\"Training for event pair: {event_pair_name}\")\n",
        "\n",
        "    # Preprocess the data for the current event pair\n",
        "    epochs_data_selected = preprocess_data(event_pair_function, epoched_data)\n",
        "\n",
        "    # Extract data and labels\n",
        "    X = epochs_data_selected.get_data()\n",
        "    y = epochs_data_selected.events[:, -1]\n",
        "\n",
        "    # Map the event codes to integer labels starting from 0\n",
        "    unique_events = np.unique(y)\n",
        "    event_to_label = {event: idx for idx, event in enumerate(unique_events)}\n",
        "    y_labels = np.array([event_to_label[event] for event in y])\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    y_one_hot = to_categorical(y_labels, num_classes=2)\n",
        "\n",
        "    # Split the data into training and testing sets (95% for training, 5% for testing)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.05, random_state=42, stratify=y_one_hot)\n",
        "\n",
        "\n",
        "    # Print class distribution for training set\n",
        "    train_class_distribution = pd.Series(y_train.argmax(axis=1)).value_counts(normalize=True)\n",
        "    print(\"Class Distribution in Training Set:\")\n",
        "    print(train_class_distribution)\n",
        "\n",
        "    # Print class distribution for testing set\n",
        "    test_class_distribution = pd.Series(y_test.argmax(axis=1)).value_counts(normalize=True)\n",
        "    print(\"Class Distribution in Testing Set:\")\n",
        "    print(test_class_distribution)\n",
        "\n",
        "    # Normalize the data using min-max scaling\n",
        "    X_train = min_max_scaler(X_train)\n",
        "    X_test = min_max_scaler(X_test)\n",
        "\n",
        "    # Reshape the data to match EEGNet input format\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
        "\n",
        "\n",
        "    # Update the nb_classes parameter in the EEGNet function\n",
        "    nb_classes = y_train.shape[1]\n",
        "\n",
        "    # Initialize variables to store the highest AUC score and corresponding kl, d values for the current event pair\n",
        "    highest_auc_score = 0\n",
        "    highest_acc = 0\n",
        "    f1 = 0\n",
        "    best_d = None\n",
        "\n",
        "    # Iterate over the kl and d values\n",
        "    # for kl in kl_values:# Only for MEGNet/EEGNet\n",
        "    for d in d_values:\n",
        "        print()\n",
        "        # print(f\"Current kl: {kl}\")\n",
        "        print(f\"Current d: {d}\")\n",
        "        print()\n",
        "\n",
        "        # Configure the ShallowConvNet/DeepConvNet model\n",
        "        model = ShallowConvNet(nb_classes=nb_classes, Chans=X_train.shape[1], Samples=X_train.shape[2], dropoutRate=d)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        # Set a valid path for model checkpoints\n",
        "        checkpointer = ModelCheckpoint(filepath='/tmp/checkpoint.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "        # Class weights\n",
        "        class_weights = {0: 1, 1: 1}\n",
        "\n",
        "        # Fit the model to the training data\n",
        "        history = model.fit(X_train, y_train, batch_size=16, epochs=n_epochs, verbose=2,\n",
        "                            validation_split=0.2, callbacks=[checkpointer, EarlyStopping(patience=5)],\n",
        "                            class_weight=class_weights)\n",
        "\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        y_pred = model.predict(X_test)\n",
        "        auc_score = roc_auc_score(y_test, y_pred)\n",
        "        acc = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "        f1_score_val = f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='macro')\n",
        "\n",
        "        # Print the Accuracy, AUC and F1 scores\n",
        "        print('Accuracy Score:', acc)\n",
        "        print('AUC Accuracy Score:', auc_score)\n",
        "        print('F1 Score:', f1_score_val)\n",
        "\n",
        "        # Check if the current AUC score is higher than the highest recorded score for the current event pair\n",
        "        if acc > highest_acc:\n",
        "            highest_auc_score = auc_score\n",
        "            best_d = d\n",
        "            f1 = f1_score_val\n",
        "            highest_acc = acc\n",
        "\n",
        "        # Reset the Keras session to clear the model\n",
        "        K.clear_session()\n",
        "\n",
        "        print()\n",
        "        print('=' * 100)\n",
        "        print('=' * 100)\n",
        "        print()\n",
        "\n",
        "    # Append the best parameters and scores for the current event pair to the results DataFrame\n",
        "    results_df = results_df.append({'Event Pair': event_pair_name, 'Best d': best_d,\n",
        "                                    'Best Accuracy':highest_acc,'Best AUC Score': highest_auc_score, 'Best F1 Score': f1},\n",
        "                                    ignore_index=True)\n",
        "\n",
        "# Print the results DataFrame\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "IumIoW1MW08s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghFq2U2skRxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}